{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwvVYjowk92Q",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Error metrics\n",
    "error_metrics = {\n",
    "    'MSE': mean_squared_error,\n",
    "    'rMSE': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    'relative': lambda y_true, y_pred: np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "    'relativeSE': lambda y_true, y_pred: np.mean(np.square((y_true - y_pred) / y_true)) * 100,\n",
    "    'absoluteSE': mean_absolute_error,\n",
    "    'statistical correlation': r2_score\n",
    "}"
   ],
   "metadata": {
    "id": "taEvPHS-lef1",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def perform_grid_search(model, param_grid, X_train, y_train, X_test, y_test, model_name):\n",
    "    grd = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error',\n",
    "                       verbose=2, n_jobs=-1)\n",
    "    grd.fit(X_train, y_train)\n",
    "    best = grd.best_params_\n",
    "    print(f\"Best Parameters for {model_name}:\", best)\n",
    "\n",
    "    model_ = model.set_params(**best)\n",
    "    model_.fit(X_train, y_train)\n",
    "    y_pred = model_.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error for {model_name} (Best Model):\", mse)\n",
    "\n",
    "    return model_, mse, y_pred"
   ],
   "metadata": {
    "id": "O0VD5Tfeljs7"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def model_selection(models, X_train, y_train, X_test, y_test, error_metrics):\n",
    "    model_errors = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        errors = {}\n",
    "        for error_name, error_func in error_metrics.items():\n",
    "            errors[error_name] = error_func(y_test, y_pred)\n",
    "\n",
    "        model_errors[model_name] = errors\n",
    "\n",
    "    return model_errors"
   ],
   "metadata": {
    "id": "_ystLmBflljP"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def run_final(X_train, X_test, y_train, y_test, validation, validation_ids, dir_path):\n",
    "    # Support Vector Machine\n",
    "    svm_model = SVR()\n",
    "    svm_param_grid = {'C':  [0.7, 0.75, 0.8]}\n",
    "    svm_best_model, svm_mse, svm_y_pred = perform_grid_search(svm_model,\n",
    "                                                              svm_param_grid,\n",
    "                                                              X_train,\n",
    "                                                              y_train,\n",
    "                                                              X_test,\n",
    "                                                              y_test,\n",
    "                                                              'SVM')\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [30, 35, 40, 50],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1],\n",
    "    }\n",
    "    xgb_best_model, xgb_mse, xgb_y_pred = perform_grid_search(xgb_model,\n",
    "                                                              xgb_param_grid,\n",
    "                                                              X_train,\n",
    "                                                              y_train,\n",
    "                                                              X_test,\n",
    "                                                              y_test,\n",
    "                                                              'XGBoost')\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    rf_param_grid = {\n",
    "        # estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [6, 8, 10, None],\n",
    "        'min_samples_leaf': [3, 4, 5, 6, 7],\n",
    "        # 'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    rf_best_model, rf_mse, rf_y_pred = perform_grid_search(rf_model,\n",
    "                                                           rf_param_grid,\n",
    "                                                           X_train,\n",
    "                                                           y_train,\n",
    "                                                           X_test,\n",
    "                                                           y_test,\n",
    "                                                           'Random Forest')\n",
    "    # Ridge Regression\n",
    "    ridge_regression_model = Ridge()\n",
    "    ridge_param_grid = {'alpha': [0.1, 0.3, 0.5, 0.6, 0.7, 0.8,\n",
    "                                  1, 2, 4, 5, 6, 10]}\n",
    "    ridge_best_model, ridge_mse, ridge_y_pred = perform_grid_search(ridge_regression_model,\n",
    "                                                                    ridge_param_grid,\n",
    "                                                                    X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_test, y_test,\n",
    "                                                                    'Ridge Regression')\n",
    "    # Models\n",
    "    models = {\n",
    "        'SVM': svm_best_model,\n",
    "        'XGBoost': xgb_best_model,\n",
    "        'Random Forest': rf_best_model,\n",
    "        'Ridge Regression': ridge_best_model,\n",
    "    }\n",
    "\n",
    "    # Perform model selection\n",
    "    model_errors = model_selection(models, X_train, y_train,\n",
    "                                   X_test, y_test, error_metrics)\n",
    "\n",
    "    # Save model errors\n",
    "    model_errors_df = pd.DataFrame(model_errors)\n",
    "    model_errors_df.to_csv(os.path.join(dir_path, 'model_errors.csv'),\n",
    "                           index=False)\n",
    "\n",
    "    svm_y_pred = svm_best_model.predict(X_test)\n",
    "    xgb_y_pred = xgb_best_model.predict(X_test)\n",
    "    rf_y_pred = rf_best_model.predict(X_test)\n",
    "    ridge_y_pred = ridge_best_model.predict(X_test)\n",
    "\n",
    "    # Stacking\n",
    "    ensemble = (svm_y_pred + xgb_y_pred + rf_y_pred + ridge_y_pred) / 4\n",
    "\n",
    "    for error in error_metrics:\n",
    "        error_rate = error_metrics[error](y_test, ensemble)\n",
    "        print(f\"Ensemble {error}:\", error_rate)\n",
    "\n",
    "    best_models_3 = sorted(model_errors.items(), key=lambda x: x[1]['MSE'])[:3]\n",
    "    best_models_3 = [model[0] for model in best_models_3]\n",
    "    ensemble_y_pred_3 = np.zeros(len(y_test))\n",
    "    for model_name in best_models_3:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        ensemble_y_pred_3 += y_pred / 3\n",
    "\n",
    "    for error in error_metrics:\n",
    "        error_rate = error_metrics[error](y_test, ensemble_y_pred_3)\n",
    "        print(f\"Ensemble_3 {error}:\", error_rate)\n",
    "\n",
    "    best_models_2 = sorted(model_errors.items(), key=lambda x: x[1]['MSE'])[:3]\n",
    "    best_models_2 = [model[0] for model in best_models_2]\n",
    "    ensemble_y_pred_2 = np.zeros(len(y_test))\n",
    "    for model_name in best_models_2:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        ensemble_y_pred_2 += y_pred / 2\n",
    "\n",
    "    for error in error_metrics:\n",
    "        error_rate = error_metrics[error](y_test, ensemble_y_pred_2)\n",
    "        print(f\"Ensemble_2 {error}:\", error_rate)\n",
    "\n",
    "    end_preds_df = pd.DataFrame()\n",
    "    end_preds_df['id'] = validation_ids\n",
    "\n",
    "    svm_y_pred = svm_best_model.predict(validation)\n",
    "    xgb_y_pred = xgb_best_model.predict(validation)\n",
    "    rf_y_pred = rf_best_model.predict(validation)\n",
    "    ridge_y_pred = ridge_best_model.predict(validation)\n",
    "    ensemble = (svm_y_pred + xgb_y_pred + rf_y_pred + ridge_y_pred) / 4\n",
    "    end_preds_df['score'] = ensemble\n",
    "    end_preds_df.to_csv(os.path.join(dir_path, 'ensemble.csv'), index=False)\n",
    "\n",
    "    ensemble_y_pred_3 = np.zeros(len(validation))\n",
    "    for model_name in best_models_3:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(validation)\n",
    "        ensemble_y_pred_3 += y_pred / 3\n",
    "    end_preds_df['score'] = ensemble_y_pred_3\n",
    "    end_preds_df.to_csv(os.path.join(dir_path, 'ensemble_3.csv'), index=False)\n",
    "\n",
    "    ensemble_y_pred_2 = np.zeros(len(validation))\n",
    "    for model_name in best_models_2:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(validation)\n",
    "        ensemble_y_pred_2 += y_pred / 2\n",
    "    end_preds_df['score'] = ensemble_y_pred_2\n",
    "    end_preds_df.to_csv(os.path.join(dir_path, 'ensemble_2.csv'), index=False)"
   ],
   "metadata": {
    "id": "XBQxiDE5ltsl"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "id": "t5eaS2eLm2kB"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def dataset_transform(X, y, validation, test_size=0.2, random_state=42, preprocessing='StandardScaler', preprocessing_params=None, dim_reduction=None,\n",
    "                      dim_reduction_params=None):\n",
    "\n",
    "    data = pd.merge(X, y, on='id')\n",
    "    data = data.drop(['id'], axis=1)\n",
    "\n",
    "    validation_id = validation['id']\n",
    "    validation = validation.drop(['id'], axis=1)\n",
    "\n",
    "    train = data.drop(['score'], axis=1)\n",
    "    y_ = data['score']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, y_,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    preprocessing_steps = []\n",
    "    preprocessing_params = preprocessing_params or {}\n",
    "    scaler = {\n",
    "        'StandardScaler': StandardScaler(**preprocessing_params),\n",
    "        'MinMaxScaler': MinMaxScaler(**preprocessing_params),\n",
    "        'RobustScaler': RobustScaler(**preprocessing_params),\n",
    "        'Normalizer': Normalizer(**preprocessing_params),\n",
    "        'QuantileTransformer': QuantileTransformer(**preprocessing_params),\n",
    "        'PowerTransformer': PowerTransformer(**preprocessing_params),\n",
    "        'PolynomialFeatures': PolynomialFeatures(**preprocessing_params),\n",
    "    }.get(preprocessing)\n",
    "    if scaler:\n",
    "        preprocessing_steps.append(('scaler', scaler))\n",
    "\n",
    "    dim_reduction_params = dim_reduction_params or {}\n",
    "    reducer = {\n",
    "        'PCA': PCA(**dim_reduction_params),\n",
    "        'KernelPCA': KernelPCA(**dim_reduction_params),\n",
    "        'SparsePCA': SparsePCA(**dim_reduction_params),\n",
    "        'TruncatedSVD': TruncatedSVD(**dim_reduction_params),\n",
    "        'FactorAnalysis': FactorAnalysis(**dim_reduction_params),\n",
    "    }.get(dim_reduction)\n",
    "    if reducer:\n",
    "        preprocessing_steps.append(('reducer', reducer))\n",
    "\n",
    "    if preprocessing_steps:\n",
    "        pipeline = Pipeline(steps=preprocessing_steps)\n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.transform(X_test)\n",
    "        validation = pipeline.transform(validation)\n",
    "    else:\n",
    "        raise ValueError(\"No valid preprocessing or dimensionality reduction method provided\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    dir_path = f\"results/{preprocessing}_{dim_reduction}/\"\n",
    "    return X_train, X_test, y_train, y_test, validation, validation_id, dir_path"
   ],
   "metadata": {
    "id": "8hu_f9GTmuhR"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    X_train = pd.read_csv('pc_X_train.csv')\n",
    "    y_train = pd.read_csv('pc_y_train.csv')\n",
    "    validation = pd.read_csv('pc_X_test.csv')\n",
    "\n",
    "    X_train, X_test, y_train, y_test, validation, validation_id, dir_path = dataset_transform(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        validation=validation,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        preprocessing='PowerTransformer', # StandardScaler, MinMaxScaler, RobustScaler, Normalizer, QuantileTransformer, PowerTransformer, PolynomialFeatures\n",
    "        preprocessing_params={'method': 'yeo-johnson', 'standardize': False},\n",
    "        dim_reduction='SparsePCA',  # PCA, KernelPCA, SparsePCA, TruncatedSVD, FactorAnalysis\n",
    "        dim_reduction_params={},\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "    run_final(X_train, X_test, y_train, y_test, validation, validation_id, dir_path)"
   ],
   "metadata": {
    "id": "kzarWjnimeaV"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "159oBk5YnA2z",
    "outputId": "a86e9df6-ab3e-4212-ee9c-919fb6cd730c"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 647 iterations, alpha=2.505e-03, previous alpha=2.505e-03, with an active set of 272 regressors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-263240bbee7e>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-30-ea40bead76d6>\u001B[0m in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mvalidation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'pc_X_test.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     X_train, X_test, y_train, y_test, validation, validation_id, dir_path = dataset_transform(\n\u001B[0m\u001B[1;32m      7\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-29-8b136422a9f7>\u001B[0m in \u001B[0;36mdataset_transform\u001B[0;34m(X, y, validation, test_size, random_state, preprocessing, preprocessing_params, dim_reduction, dim_reduction_params)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mpreprocessing_steps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpreprocessing_steps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m         \u001B[0mX_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0mX_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[0mvalidation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalidation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    443\u001B[0m             \u001B[0mfit_params_last_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_params_steps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    444\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlast_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"fit_transform\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 445\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mlast_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params_last_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    446\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    447\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mlast_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params_last_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001B[0m in \u001B[0;36mwrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m         \u001B[0mdata_to_wrap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    141\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_to_wrap\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m             \u001B[0;31m# only wrap the first output for cross decomposition\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    876\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    877\u001B[0m             \u001B[0;31m# fit method of arity 1 (unsupervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 878\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    879\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    880\u001B[0m             \u001B[0;31m# fit method of arity 2 (supervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_sparse_pca.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     83\u001B[0m             \u001B[0mn_components\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_components\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_components\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_sparse_pca.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, X, n_components, random_state)\u001B[0m\n\u001B[1;32m    305\u001B[0m         \u001B[0mcode_init\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mV_init\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mV_init\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m         \u001B[0mdict_init\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mU_init\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mU_init\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 307\u001B[0;31m         code, dictionary, E, self.n_iter_ = dict_learning(\n\u001B[0m\u001B[1;32m    308\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m             \u001B[0mn_components\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_dict_learning.py\u001B[0m in \u001B[0;36mdict_learning\u001B[0;34m(X, n_components, alpha, max_iter, tol, method, n_jobs, dict_init, code_init, callback, verbose, random_state, return_n_iter, positive_dict, positive_code, method_max_iter)\u001B[0m\n\u001B[1;32m    683\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    684\u001B[0m         \u001B[0;31m# Update code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 685\u001B[0;31m         code = sparse_encode(\n\u001B[0m\u001B[1;32m    686\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    687\u001B[0m             \u001B[0mdictionary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_dict_learning.py\u001B[0m in \u001B[0;36msparse_encode\u001B[0;34m(X, dictionary, gram, cov, algorithm, n_nonzero_coefs, alpha, copy_cov, init, max_iter, n_jobs, check_input, verbose, positive)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0meffective_n_jobs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0malgorithm\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"threshold\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 378\u001B[0;31m         code = _sparse_encode(\n\u001B[0m\u001B[1;32m    379\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m             \u001B[0mdictionary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_dict_learning.py\u001B[0m in \u001B[0;36m_sparse_encode\u001B[0;34m(X, dictionary, gram, cov, algorithm, regularization, copy_cov, init, max_iter, check_input, verbose, positive)\u001B[0m\n\u001B[1;32m    154\u001B[0m                 \u001B[0mmax_iter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_iter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m             )\n\u001B[0;32m--> 156\u001B[0;31m             \u001B[0mlasso_lars\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdictionary\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mXy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcov\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    157\u001B[0m             \u001B[0mnew_code\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlasso_lars\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoef_\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, Xy)\u001B[0m\n\u001B[1;32m   1142\u001B[0m             \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnoise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1144\u001B[0;31m         self._fit(\n\u001B[0m\u001B[1;32m   1145\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m             \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, X, y, max_iter, alpha, fit_path, normalize, Xy)\u001B[0m\n\u001B[1;32m   1075\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_targets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1076\u001B[0m                 \u001B[0mthis_Xy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mXy\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mXy\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1077\u001B[0;31m                 alphas, _, self.coef_[k], n_iter_ = lars_path(\n\u001B[0m\u001B[1;32m   1078\u001B[0m                     \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1079\u001B[0m                     \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py\u001B[0m in \u001B[0;36mlars_path\u001B[0;34m(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001B[0m\n\u001B[1;32m    168\u001B[0m             \u001B[0;34m\"Use lars_path_gram to avoid passing X and y.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m         )\n\u001B[0;32m--> 170\u001B[0;31m     return _lars_path_solver(\n\u001B[0m\u001B[1;32m    171\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m         \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py\u001B[0m in \u001B[0;36m_lars_path_solver\u001B[0;34m(X, y, Xy, Gram, n_samples, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001B[0m\n\u001B[1;32m    723\u001B[0m             \u001B[0;31m# think could be avoided if we just update it using an\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m             \u001B[0;31m# orthogonal (QR) decomposition of X\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 725\u001B[0;31m             \u001B[0mcorr_eq_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mGram\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mn_active\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_active\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mleast_squares\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    726\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    727\u001B[0m         \u001B[0;31m# Explicit rounding can be necessary to avoid `np.argmax(Cov)` yielding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001B[0m in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ]
}
