{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZwvVYjowk92Q",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.872802500Z",
     "start_time": "2024-01-09T21:49:20.866366100Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Error metrics\n",
    "error_metrics = {\n",
    "    'MSE': mean_squared_error,\n",
    "    'rMSE': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    'relative': lambda y_true, y_pred: np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "    'relativeSE': lambda y_true, y_pred: np.mean(np.square((y_true - y_pred) / y_true)) * 100,\n",
    "    'absoluteSE': mean_absolute_error,\n",
    "    'statistical correlation': r2_score\n",
    "}"
   ],
   "metadata": {
    "id": "taEvPHS-lef1",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.898822600Z",
     "start_time": "2024-01-09T21:49:20.872802500Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def perform_grid_search(model, param_grid, X_train, y_train, X_test, y_test, model_name):\n",
    "    grd = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error',\n",
    "                       verbose=2, n_jobs=-1)\n",
    "    grd.fit(X_train, y_train)\n",
    "    best = grd.best_params_\n",
    "    print(f\"Best Parameters for {model_name}:\", best)\n",
    "\n",
    "    model_ = model.set_params(**best)\n",
    "    model_.fit(X_train, y_train)\n",
    "    y_pred = model_.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error for {model_name} (Best Model):\", mse)\n",
    "\n",
    "    return model_, mse, y_pred"
   ],
   "metadata": {
    "id": "O0VD5Tfeljs7",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.899176400Z",
     "start_time": "2024-01-09T21:49:20.883885Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def model_selection(models, X_train, y_train, X_test, y_test, error_metrics):\n",
    "    model_errors = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        errors = {}\n",
    "        for error_name, error_func in error_metrics.items():\n",
    "            errors[error_name] = error_func(y_test, y_pred)\n",
    "\n",
    "        model_errors[model_name] = errors\n",
    "\n",
    "    return model_errors"
   ],
   "metadata": {
    "id": "_ystLmBflljP",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.906044300Z",
     "start_time": "2024-01-09T21:49:20.892087800Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def run_final(X_train, X_test, y_train, y_test, validation, validation_ids, dir_path):\n",
    "    # Support Vector Machine\n",
    "    svm_model = SVR()\n",
    "    svm_param_grid = {'C':  [0.7, 0.75, 0.8]}\n",
    "    svm_best_model, svm_mse, svm_y_pred = perform_grid_search(svm_model,\n",
    "                                                              svm_param_grid,\n",
    "                                                              X_train,\n",
    "                                                              y_train,\n",
    "                                                              X_test,\n",
    "                                                              y_test,\n",
    "                                                              'SVM')\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [30, 35, 40, 50],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1],\n",
    "    }\n",
    "    xgb_best_model, xgb_mse, xgb_y_pred = perform_grid_search(xgb_model,\n",
    "                                                              xgb_param_grid,\n",
    "                                                              X_train,\n",
    "                                                              y_train,\n",
    "                                                              X_test,\n",
    "                                                              y_test,\n",
    "                                                              'XGBoost')\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    rf_param_grid = {\n",
    "        # estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [2, 6, 8, 10, None],\n",
    "        'min_samples_leaf': [3, 4, 5, 6, 7],\n",
    "        # 'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    rf_best_model, rf_mse, rf_y_pred = perform_grid_search(rf_model,\n",
    "                                                           rf_param_grid,\n",
    "                                                           X_train,\n",
    "                                                           y_train,\n",
    "                                                           X_test,\n",
    "                                                           y_test,\n",
    "                                                           'Random Forest')\n",
    "    # Ridge Regression\n",
    "    ridge_regression_model = Ridge()\n",
    "    ridge_param_grid = {'alpha': [0.1, 0.3, 0.5, 0.6, 0.7, 0.8,\n",
    "                                  1, 2, 4, 5, 6, 10]}\n",
    "    ridge_best_model, ridge_mse, ridge_y_pred = perform_grid_search(ridge_regression_model,\n",
    "                                                                    ridge_param_grid,\n",
    "                                                                    X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_test, y_test,\n",
    "                                                                    'Ridge Regression')\n",
    "    # Models\n",
    "    models = {\n",
    "        'SVM': svm_best_model,\n",
    "        'XGBoost': xgb_best_model,\n",
    "        'Random Forest': rf_best_model,\n",
    "        'Ridge Regression': ridge_best_model,\n",
    "    }\n",
    "\n",
    "    # Perform model selection\n",
    "    model_errors = model_selection(models, X_train, y_train,\n",
    "                                   X_test, y_test, error_metrics)\n",
    "\n",
    "    # Save model errors\n",
    "    model_errors_df = pd.DataFrame(model_errors)\n",
    "    model_errors_df.to_csv(os.path.join(dir_path, 'model_errors.csv'),\n",
    "                           index=False)\n",
    "\n",
    "    svm_y_pred = svm_best_model.predict(X_test)\n",
    "    xgb_y_pred = xgb_best_model.predict(X_test)\n",
    "    rf_y_pred = rf_best_model.predict(X_test)\n",
    "    ridge_y_pred = ridge_best_model.predict(X_test)\n",
    "\n",
    "    # Stacking\n",
    "    ensemble = (svm_y_pred + xgb_y_pred + rf_y_pred + ridge_y_pred) / 4\n",
    "\n",
    "    for error in error_metrics:\n",
    "        error_rate = error_metrics[error](y_test, ensemble)\n",
    "        print(f\"Ensemble {error}:\", error_rate)\n",
    "\n",
    "    best_models_3 = sorted(model_errors.items(), key=lambda x: x[1]['MSE'])[:3]\n",
    "    best_models_3 = [model[0] for model in best_models_3]\n",
    "    ensemble_y_pred_3 = np.zeros(len(y_test))\n",
    "    for model_name in best_models_3:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        ensemble_y_pred_3 += y_pred / 3\n",
    "\n",
    "    for error in error_metrics:\n",
    "        error_rate = error_metrics[error](y_test, ensemble_y_pred_3)\n",
    "        print(f\"Ensemble_3 {error}:\", error_rate)\n",
    "\n",
    "    best_models_2 = sorted(model_errors.items(), key=lambda x: x[1]['MSE'])[:3]\n",
    "    best_models_2 = [model[0] for model in best_models_2]\n",
    "    ensemble_y_pred_2 = np.zeros(len(y_test))\n",
    "    for model_name in best_models_2:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        ensemble_y_pred_2 += y_pred / 2\n",
    "\n",
    "    for error in error_metrics:\n",
    "        error_rate = error_metrics[error](y_test, ensemble_y_pred_2)\n",
    "        print(f\"Ensemble_2 {error}:\", error_rate)\n",
    "\n",
    "    end_preds_df = pd.DataFrame()\n",
    "    end_preds_df['id'] = validation_ids\n",
    "\n",
    "    svm_y_pred = svm_best_model.predict(validation)\n",
    "    xgb_y_pred = xgb_best_model.predict(validation)\n",
    "    rf_y_pred = rf_best_model.predict(validation)\n",
    "    ridge_y_pred = ridge_best_model.predict(validation)\n",
    "    ensemble = (svm_y_pred + xgb_y_pred + rf_y_pred + ridge_y_pred) / 4\n",
    "    end_preds_df['score'] = ensemble\n",
    "    end_preds_df.to_csv(os.path.join(dir_path, 'ensemble.csv'), index=False)\n",
    "\n",
    "    ensemble_y_pred_3 = np.zeros(len(validation))\n",
    "    for model_name in best_models_3:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(validation)\n",
    "        ensemble_y_pred_3 += y_pred / 3\n",
    "    end_preds_df['score'] = ensemble_y_pred_3\n",
    "    end_preds_df.to_csv(os.path.join(dir_path, 'ensemble_3.csv'), index=False)\n",
    "\n",
    "    ensemble_y_pred_2 = np.zeros(len(validation))\n",
    "    for model_name in best_models_2:\n",
    "        model = models[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(validation)\n",
    "        ensemble_y_pred_2 += y_pred / 2\n",
    "    end_preds_df['score'] = ensemble_y_pred_2\n",
    "    end_preds_df.to_csv(os.path.join(dir_path, 'ensemble_2.csv'), index=False)"
   ],
   "metadata": {
    "id": "XBQxiDE5ltsl",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.920152400Z",
     "start_time": "2024-01-09T21:49:20.907484700Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "id": "t5eaS2eLm2kB",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.920152400Z",
     "start_time": "2024-01-09T21:49:20.912827Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def dataset_transform(X, y, validation, test_size=0.2, random_state=42, preprocessing='StandardScaler', preprocessing_params=None, dim_reduction=None,\n",
    "                      dim_reduction_params=None):\n",
    "\n",
    "    data = pd.merge(X, y, on='id')\n",
    "    data = data.drop(['id'], axis=1)\n",
    "\n",
    "    validation_id = validation['id']\n",
    "    validation = validation.drop(['id'], axis=1)\n",
    "\n",
    "    train = data.drop(['score'], axis=1)\n",
    "    y_ = data['score']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, y_,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    preprocessing_steps = []\n",
    "    preprocessing_params = preprocessing_params or {}\n",
    "    scaler = {\n",
    "        'StandardScaler': StandardScaler(**preprocessing_params),\n",
    "        'MinMaxScaler': MinMaxScaler(**preprocessing_params),\n",
    "        'RobustScaler': RobustScaler(**preprocessing_params),\n",
    "        'Normalizer': Normalizer(**preprocessing_params),\n",
    "        'QuantileTransformer': QuantileTransformer(**preprocessing_params),\n",
    "        'PowerTransformer': PowerTransformer(**preprocessing_params),\n",
    "        'PolynomialFeatures': PolynomialFeatures(**preprocessing_params),\n",
    "    }.get(preprocessing)\n",
    "    if scaler:\n",
    "        preprocessing_steps.append(('scaler', scaler))\n",
    "\n",
    "    dim_reduction_params = dim_reduction_params or {}\n",
    "    reducer = {\n",
    "        'PCA': PCA(**dim_reduction_params),\n",
    "        'KernelPCA': KernelPCA(**dim_reduction_params),\n",
    "        'SparsePCA': SparsePCA(**dim_reduction_params),\n",
    "        'TruncatedSVD': TruncatedSVD(**dim_reduction_params),\n",
    "        'FactorAnalysis': FactorAnalysis(**dim_reduction_params),\n",
    "    }.get(dim_reduction)\n",
    "    if reducer:\n",
    "        preprocessing_steps.append(('reducer', reducer))\n",
    "\n",
    "    if preprocessing_steps:\n",
    "        pipeline = Pipeline(steps=preprocessing_steps)\n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.transform(X_test)\n",
    "        validation = pipeline.transform(validation)\n",
    "    else:\n",
    "        raise ValueError(\"No valid preprocessing or dimensionality reduction method provided\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    dir_path = f\"results/{preprocessing}_{dim_reduction}/\"\n",
    "    return X_train, X_test, y_train, y_test, validation, validation_id, dir_path"
   ],
   "metadata": {
    "id": "8hu_f9GTmuhR",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.928398300Z",
     "start_time": "2024-01-09T21:49:20.920152400Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    X_train = pd.read_csv('pc_X_train.csv')\n",
    "    y_train = pd.read_csv('pc_y_train.csv')\n",
    "    validation = pd.read_csv('pc_X_test.csv')\n",
    "\n",
    "    X_train, X_test, y_train, y_test, validation, validation_id, dir_path = dataset_transform(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        validation=validation,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        preprocessing='PowerTransformer', # StandardScaler, MinMaxScaler, RobustScaler, Normalizer, QuantileTransformer, PowerTransformer, PolynomialFeatures\n",
    "        preprocessing_params={},\n",
    "        dim_reduction='SparsePCA',  # PCA, KernelPCA, SparsePCA, TruncatedSVD, FactorAnalysis\n",
    "        dim_reduction_params={},\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "    run_final(X_train, X_test, y_train, y_test, validation, validation_id, dir_path)"
   ],
   "metadata": {
    "id": "kzarWjnimeaV",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:49:20.935701600Z",
     "start_time": "2024-01-09T21:49:20.927251700Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "159oBk5YnA2z",
    "outputId": "b097968e-a95b-47c5-bbb1-5b5fe79836d9",
    "ExecuteTime": {
     "end_time": "2024-01-09T22:03:00.740759500Z",
     "start_time": "2024-01-09T21:49:20.934619700Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Csana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "C:\\Users\\Csana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1580, 468)\n",
      "X_test shape: (396, 468)\n",
      "y_train shape: (1580,)\n",
      "y_test shape: (396,)\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Parameters for SVM: {'C': 0.75}\n",
      "Mean Squared Error for SVM (Best Model): 0.40683627500306063\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters for XGBoost: {'max_depth': 2, 'n_estimators': 30, 'subsample': 1}\n",
      "Mean Squared Error for XGBoost (Best Model): 0.40544378264792474\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Best Parameters for Random Forest: {'max_depth': 8, 'min_samples_leaf': 6}\n",
      "Mean Squared Error for Random Forest (Best Model): 0.4000606175939326\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters for Ridge Regression: {'alpha': 10}\n",
      "Mean Squared Error for Ridge Regression (Best Model): 0.4070181115297032\n",
      "Ensemble MSE: 0.3828287347967014\n",
      "Ensemble rMSE: 0.6187315530960914\n",
      "Ensemble relative: 15.651057319527395\n",
      "Ensemble relativeSE: 7.0185947492904255\n",
      "Ensemble absoluteSE: 0.47274438191039125\n",
      "Ensemble statistical correlation: 0.6057186615359185\n",
      "Ensemble_3 MSE: 0.387310328609594\n",
      "Ensemble_3 rMSE: 0.6223426135253748\n",
      "Ensemble_3 relative: 15.717153532842854\n",
      "Ensemble_3 relativeSE: 7.256132435065334\n",
      "Ensemble_3 absoluteSE: 0.4720486299206948\n",
      "Ensemble_3 statistical correlation: 0.6011029975421014\n",
      "Ensemble_2 MSE: 4.092694435316213\n",
      "Ensemble_2 rMSE: 2.0230408881968285\n",
      "Ensemble_2 relative: 57.72626644165542\n",
      "Ensemble_2 relativeSE: 49.01100400081081\n",
      "Ensemble_2 absoluteSE: 1.8925855515352623\n",
      "Ensemble_2 statistical correlation: -3.215130404821638\n"
     ]
    }
   ]
  }
 ]
}
